{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import data_loading_code as pre\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    " # get data, pre-process and split\n",
    "data = pre.pd.read_csv(\"amazon_cells_labelled.txt\", delimiter='\\t', header=None)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Split the DataFrame into 80% train and 20% test\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "#training and validation\n",
    "train_df.columns = ['Sentence', 'Class']\n",
    "train_df['index'] = train_df.index                                          # add new column index\n",
    "columns = ['index', 'Class', 'Sentence']\n",
    "train_df = pre.preprocess_pandas(train_df, columns)                            # pre-process\n",
    "\n",
    "training_data, validation_data, training_labels, validation_labels = pre.train_test_split( # split the data into training, validation, and test splits\n",
    "    train_df['Sentence'].values.astype('U'),\n",
    "    train_df['Class'].values.astype('int32'),\n",
    "    test_size=0.10,\n",
    "    random_state=0,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_df.columns = ['Sentence', 'Class']\n",
    "test_df['index'] = test_df.index                                          # add new column index\n",
    "columns = ['index', 'Class', 'Sentence']\n",
    "test_df = pre.preprocess_pandas(test_df, columns) \n",
    "\n",
    "test_data = test_df['Sentence'].values.astype('U')\n",
    "test_labels = test_df['Class'].values.astype('int32')\n",
    "\n",
    "\n",
    "# vectorize data using TFIDF and transform for PyTorch for scalability\n",
    "word_vectorizer = pre.TfidfVectorizer(analyzer='word', ngram_range=(1,2), max_features=50000, max_df=0.5, use_idf=True, norm='l2')\n",
    "training_data = word_vectorizer.fit_transform(training_data)        # transform texts to sparse matrix\n",
    "training_data = training_data.todense()                             # convert to dense matrix for Pytorch\n",
    "vocab_size = len(word_vectorizer.vocabulary_)\n",
    "\n",
    "validation_data = word_vectorizer.transform(validation_data)\n",
    "validation_data = validation_data.todense()\n",
    "\n",
    "test_data = word_vectorizer.transform(test_data)        # transform texts to sparse matrix\n",
    "test_data = test_data.todense()                             # convert to dense matrix for Pytorch\n",
    "\n",
    "assert training_data.shape[1] == validation_data.shape[1] == test_data.shape[1], \"Feature mismatch in TF-IDF!\"\n",
    "\n",
    "train_x_tensor = torch.from_numpy(np.array(training_data)).type(torch.FloatTensor)\n",
    "train_y_tensor = torch.from_numpy(np.array(training_labels)).long()\n",
    "\n",
    "validation_x_tensor = torch.from_numpy(np.array(validation_data)).type(torch.FloatTensor)\n",
    "validation_y_tensor = torch.from_numpy(np.array(validation_labels)).long()\n",
    "\n",
    "test_x_tensor = torch.from_numpy(np.array(test_data)).type(torch.FloatTensor)\n",
    "test_y_tensor = torch.from_numpy(np.array(test_labels)).long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 5997])\n",
      "torch.Size([720, 5997])\n",
      "torch.Size([80, 5997])\n"
     ]
    }
   ],
   "source": [
    "print(test_x_tensor.shape)\n",
    "print(train_x_tensor.shape)\n",
    "print(validation_x_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "        nn.Linear(vocab_size,1000),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(1000,100),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100,25),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(25,2)\n",
    "        )\n",
    "    \n",
    "    def feedforward(self,input):\n",
    "        return self.network(input)\n",
    "    \n",
    "    \n",
    "def backward(model, epochs, optimizer, loss_function, train_x_tensor, train_y_tensor, validation_x_tensor, validation_y_tensor):\n",
    "    train_Acc = 0\n",
    "    train_Acc = 0\n",
    "    validation_AccV = 0\n",
    "    best_loss = 100\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        trainCorr = 0\n",
    "        valCorr = 0\n",
    "        \n",
    "        for (sentences, labels) in zip(train_x_tensor,train_y_tensor):\n",
    "            pred = model.feedforward(sentences)\n",
    "            \n",
    "            loss = loss_function(pred, labels) \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "                \n",
    "            pred = torch.argmax(F.softmax(pred))\n",
    "            trainCorr += torch.sum(pred==labels).item()\n",
    "        train_Acc = trainCorr/len(training_data)\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():   \n",
    "            for (sentencesv, labelsv) in zip(validation_x_tensor,validation_y_tensor):\n",
    "                \n",
    "                predv = model.feedforward(sentencesv)\n",
    "                lossv = loss_function(predv,labelsv)\n",
    "\n",
    "                if best_loss > lossv.item():\n",
    "                    best_loss = lossv.item()\n",
    "                    torch.save(model, \"nlpbest\")\n",
    "                \n",
    "                predv = torch.argmax(F.softmax(predv))\n",
    "                valCorr += torch.sum(predv==labelsv).item()\n",
    "            validation_AccV = valCorr/len(validation_data)\n",
    "        print(\"epoch:\", epoch, \"Loss Training:\", loss.item(), \"Training acc:\", train_Acc)\n",
    "        print(\"epoch:\", epoch, \"Loss Validation:\", lossv.item(),\"Validation acc:\", validation_AccV)\n",
    "    return model\n",
    "        \n",
    "def test_loop(model, loss_function, test_x_tensor, test_y_tensor):\n",
    "    testCorr = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():   \n",
    "        for (sentencesTest, labelsTest) in zip(test_x_tensor,test_y_tensor):\n",
    "            \n",
    "            predTest = model.feedforward(sentencesTest)\n",
    "            lossTest = loss_function(predTest,labelsTest)\n",
    "\n",
    "            if best_loss > lossTest.item():\n",
    "                best_loss = lossTest.item()\n",
    "            \n",
    "            \n",
    "            predTest = torch.argmax(F.softmax(predTest))\n",
    "            testCorr += torch.sum(predTest==labelsTest).item()\n",
    "        test_Acc = testCorr/len(test_data)\n",
    "    print( \"Loss Test:\", lossTest.item(), \"Test acc:\", test_Acc)\n",
    "\n",
    "    \n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemntation:\n",
    "\n",
    "Small data set Amaxon cell labelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harry\\AppData\\Local\\Temp\\ipykernel_61340\\2086315148.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = torch.argmax(F.softmax(pred))\n",
      "C:\\Users\\harry\\AppData\\Local\\Temp\\ipykernel_61340\\2086315148.py:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  predv = torch.argmax(F.softmax(predv))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 Loss Training: 0.09940920025110245 Training acc: 0.6736111111111112\n",
      "epoch: 0 Loss Validation: 0.2976081669330597 Validation acc: 0.8\n",
      "epoch: 1 Loss Training: 1.1920928244535389e-07 Training acc: 0.975\n",
      "epoch: 1 Loss Validation: 0.0005050813779234886 Validation acc: 0.8125\n",
      "epoch: 2 Loss Training: 6.437280717364047e-06 Training acc: 1.0\n",
      "epoch: 2 Loss Validation: 0.008211423642933369 Validation acc: 0.8\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x2164 and 5997x1000)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\harry\\Skola\\D7047E\\lab1\\lab1.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/harry/Skola/D7047E/lab1/lab1.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/harry/Skola/D7047E/lab1/lab1.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m trained_model \u001b[39m=\u001b[39m backward(model, epochs, optimizer, criterion, train_x_tensor, train_y_tensor, validation_x_tensor, validation_y_tensor)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/harry/Skola/D7047E/lab1/lab1.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m test_loop(trained_model, criterion,test_x_tensor, test_y_tensor)\n",
      "\u001b[1;32mc:\\Users\\harry\\Skola\\D7047E\\lab1\\lab1.ipynb Cell 10\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/harry/Skola/D7047E/lab1/lab1.ipynb#X12sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():   \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/harry/Skola/D7047E/lab1/lab1.ipynb#X12sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     \u001b[39mfor\u001b[39;00m (sentencesTest, labelsTest) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(test_x_tensor,test_y_tensor):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/harry/Skola/D7047E/lab1/lab1.ipynb#X12sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m         predTest \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfeedforward(sentencesTest)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/harry/Skola/D7047E/lab1/lab1.ipynb#X12sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m         lossTest \u001b[39m=\u001b[39m loss_function(predTest,labelsTest)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/harry/Skola/D7047E/lab1/lab1.ipynb#X12sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m         \u001b[39mif\u001b[39;00m best_loss \u001b[39m>\u001b[39m lossTest\u001b[39m.\u001b[39mitem():\n",
      "\u001b[1;32mc:\\Users\\harry\\Skola\\D7047E\\lab1\\lab1.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/harry/Skola/D7047E/lab1/lab1.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeedforward\u001b[39m(\u001b[39mself\u001b[39m,\u001b[39minput\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/harry/Skola/D7047E/lab1/lab1.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnetwork(\u001b[39minput\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    251\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x2164 and 5997x1000)"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "model = Net()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.001, weight_decay=0.2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "trained_model = backward(model, epochs, optimizer, criterion, train_x_tensor, train_y_tensor, validation_x_tensor, validation_y_tensor)\n",
    "\n",
    "test_loop(trained_model, criterion,test_x_tensor, test_y_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on large data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data, pre-process and split\n",
    "data = pre.pd.read_csv(\"amazon_cells_labelled_LARGE_25K.txt\", delimiter='\\t', header=None)\n",
    "data.columns = ['Sentence', 'Class']\n",
    "data['index'] = data.index                                          # add new column index\n",
    "columns = ['index', 'Class', 'Sentence']\n",
    "data = pre.preprocess_pandas(data, columns)                             # pre-process\n",
    "training_data, validation_data, training_labels, validation_labels = pre.train_test_split( # split the data into training, validation, and test splits\n",
    "    data['Sentence'].values.astype('U'),\n",
    "    data['Class'].values.astype('int32'),\n",
    "    test_size=0.10,\n",
    "    random_state=0,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# vectorize data using TFIDF and transform for PyTorch for scalability\n",
    "word_vectorizer = pre.TfidfVectorizer(analyzer='word', ngram_range=(1,2), max_features=50000, max_df=0.5, use_idf=True, norm='l2')\n",
    "training_data = word_vectorizer.fit_transform(training_data)        # transform texts to sparse matrix\n",
    "training_data = training_data.todense()                             # convert to dense matrix for Pytorch\n",
    "vocab_size = len(word_vectorizer.vocabulary_)\n",
    "validation_data = word_vectorizer.transform(validation_data)\n",
    "validation_data = validation_data.todense()\n",
    "train_x_tensor = torch.from_numpy(np.array(training_data)).type(torch.FloatTensor)\n",
    "train_y_tensor = torch.from_numpy(np.array(training_labels)).long()\n",
    "validation_x_tensor = torch.from_numpy(np.array(validation_data)).type(torch.FloatTensor)\n",
    "validation_y_tensor = torch.from_numpy(np.array(validation_labels)).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22500])\n"
     ]
    }
   ],
   "source": [
    "print(train_y_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harry\\AppData\\Local\\Temp\\ipykernel_61340\\256856433.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = torch.argmax(F.softmax(pred))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\harry\\Skola\\D7047E\\lab1\\lab1.ipynb Cell 14\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/harry/Skola/D7047E/lab1/lab1.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdamW(model\u001b[39m.\u001b[39mparameters(), lr \u001b[39m=\u001b[39m \u001b[39m0.001\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/harry/Skola/D7047E/lab1/lab1.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/harry/Skola/D7047E/lab1/lab1.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m trained_model \u001b[39m=\u001b[39m backward(model, epochs, optimizer, criterion, train_x_tensor, train_y_tensor, validation_x_tensor, validation_y_tensor)\n",
      "\u001b[1;32mc:\\Users\\harry\\Skola\\D7047E\\lab1\\lab1.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/harry/Skola/D7047E/lab1/lab1.ipynb#X16sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_function(pred, labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/harry/Skola/D7047E/lab1/lab1.ipynb#X16sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/harry/Skola/D7047E/lab1/lab1.ipynb#X16sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/harry/Skola/D7047E/lab1/lab1.ipynb#X16sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/harry/Skola/D7047E/lab1/lab1.ipynb#X16sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m pred \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(F\u001b[39m.\u001b[39msoftmax(pred))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    488\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m\"\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     92\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\adamw.py:220\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    207\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m cast(Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m], group[\u001b[39m\"\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    209\u001b[0m     has_complex \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[0;32m    210\u001b[0m         group,\n\u001b[0;32m    211\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    217\u001b[0m         state_steps,\n\u001b[0;32m    218\u001b[0m     )\n\u001b[1;32m--> 220\u001b[0m     adamw(\n\u001b[0;32m    221\u001b[0m         params_with_grad,\n\u001b[0;32m    222\u001b[0m         grads,\n\u001b[0;32m    223\u001b[0m         exp_avgs,\n\u001b[0;32m    224\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    225\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    226\u001b[0m         state_steps,\n\u001b[0;32m    227\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    228\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    229\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    230\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    231\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    232\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    233\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    234\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    235\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    236\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    237\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    238\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    239\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    240\u001b[0m         has_complex\u001b[39m=\u001b[39;49mhas_complex,\n\u001b[0;32m    241\u001b[0m     )\n\u001b[0;32m    243\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[39mreturn\u001b[39;00m disabled_func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\adamw.py:782\u001b[0m, in \u001b[0;36madamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    780\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adamw\n\u001b[1;32m--> 782\u001b[0m func(\n\u001b[0;32m    783\u001b[0m     params,\n\u001b[0;32m    784\u001b[0m     grads,\n\u001b[0;32m    785\u001b[0m     exp_avgs,\n\u001b[0;32m    786\u001b[0m     exp_avg_sqs,\n\u001b[0;32m    787\u001b[0m     max_exp_avg_sqs,\n\u001b[0;32m    788\u001b[0m     state_steps,\n\u001b[0;32m    789\u001b[0m     amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    790\u001b[0m     beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    791\u001b[0m     beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    792\u001b[0m     lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    793\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    794\u001b[0m     eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    795\u001b[0m     maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    796\u001b[0m     capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    797\u001b[0m     differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    798\u001b[0m     grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    799\u001b[0m     found_inf\u001b[39m=\u001b[39;49mfound_inf,\n\u001b[0;32m    800\u001b[0m     has_complex\u001b[39m=\u001b[39;49mhas_complex,\n\u001b[0;32m    801\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\adamw.py:427\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[0;32m    425\u001b[0m         denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m    426\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m         denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39;49m bias_correction2_sqrt)\u001b[39m.\u001b[39;49madd_(eps)\n\u001b[0;32m    429\u001b[0m     param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n\u001b[0;32m    431\u001b[0m \u001b[39m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "model = Net()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "trained_model = backward(model, epochs, optimizer, criterion, train_x_tensor, train_y_tensor, validation_x_tensor, validation_y_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
