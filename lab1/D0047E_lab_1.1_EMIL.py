# -*- coding: utf-8 -*-
"""D0047E Lab 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/110b04OJk-xWFxcYQqYzVs5p_gJ1W6SYu
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
import numpy as np
from matplotlib import pyplot
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
from nltk import word_tokenize
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, classification_report

import nltk
nltk.download('punkt_tab')



# === Load & Preprocess Data ===

def preprocess_pandas(data, columns):
    df_ = pd.DataFrame(columns=columns)
    data['Sentence'] = data['Sentence'].str.lower()
    data['Sentence'] = data['Sentence'].replace('[a-zA-Z0-9-_.]+@[a-zA-Z0-9-_.]+', '', regex=True)                      # remove emails
    data['Sentence'] = data['Sentence'].replace('((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\.|$)){4}', '', regex=True)    # remove IP address
    data['Sentence'] = data['Sentence'].str.replace('[^\w\s]','')                                                       # remove special characters
    data['Sentence'] = data['Sentence'].replace('\d', '', regex=True)                                                   # remove numbers
    for index, row in data.iterrows():
        word_tokens = word_tokenize(row['Sentence'])
        filtered_sent = [w for w in word_tokens if not w in stopwords.words('english')]
        df_.loc[len(df_)] = {
            "index": row['index'],
            "Class": row['Class'],
            "Sentence": " ".join(filtered_sent)
        }
    return data


data = pd.read_csv("amazon_cells_labelled.txt", delimiter='\t', header=None)
data.columns = ['Sentence', 'Class']
data['index'] = data.index
columns = ['index', 'Class', 'Sentence']
data = preprocess_pandas(data, columns)

# Split dataset
training_data, validation_data, training_labels, validation_labels = train_test_split(
    data['Sentence'].values.astype('U'),
    data['Class'].values.astype('int32'),
    test_size=0.10,
    random_state=0,
    shuffle=True
)

# Vectorize text using TF-IDF
word_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,2), max_features=50000, max_df=0.5, use_idf=True, norm='l2')
training_data = word_vectorizer.fit_transform(training_data).todense()
validation_data = word_vectorizer.transform(validation_data).todense()

# Convert to PyTorch tensors
train_x_tensor = torch.from_numpy(np.array(training_data)).type(torch.FloatTensor)
train_y_tensor = torch.from_numpy(np.array(training_labels)).long()
validation_x_tensor = torch.from_numpy(np.array(validation_data)).type(torch.FloatTensor)
validation_y_tensor = torch.from_numpy(np.array(validation_labels)).long()







# Define the ANN model
class ANN(nn.Module):
    def __init__(self, input_size):
        super(ANN, self).__init__()
        self.fc1 = nn.Linear(input_size, 128)  # Hidden layer with 128 neurons
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(128, 1)  # Output layer with 1 neuron
        self.sigmoid = nn.Sigmoid()   # Sigmoid activation for probability output

    def forward(self, x):
        x = self.fc1(x)          # Pass through first layer
        x = self.relu(x)         # Apply ReLU activation
        x = self.fc2(x)          # Pass through second layer (output layer)
        x = self.sigmoid(x)      # Apply Sigmoid activation on the output
        return x


# Model, loss function, optimizer
vocab_size = len(word_vectorizer.vocabulary_)
batch_size = 64  # You can adjust the batch size
train_dataset = TensorDataset(train_x_tensor, train_y_tensor)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)


input_size = vocab_size  # Same as TF-IDF feature size
model = ANN(input_size)
criterion = nn.BCELoss()  # Binary cross-entropy loss
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
num_epochs = 50
for epoch in range(num_epochs):
    for batch_x, batch_y in train_loader:
        optimizer.zero_grad()
        outputs = model(batch_x)
        loss = criterion(outputs.squeeze(), batch_y.float())
        loss.backward()
        optimizer.step()

    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')

# Evaluate on validation set
with torch.no_grad():
    val_outputs = model(validation_x_tensor)
    val_preds = (val_outputs > 0.5).float()
    accuracy = (val_preds == validation_y_tensor.unsqueeze(1)).float().mean().item()
    print(f'Validation Accuracy: {100*accuracy:.4f}%')