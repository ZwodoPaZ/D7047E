{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import data_loading_code as pre\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, classification_report\n",
    "from sys import getsizeof\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " # get data, pre-process and split\n",
    "data = pre.pd.read_csv(\"amazon_cells_labelled.txt\", delimiter='\\t', header=None) #read the file and preprocess it, for dataframe. new datastructure\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Split the DataFrame into 80% train and 20% test\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "#training and validation\n",
    "train_df.columns = ['Sentence', 'Class']\n",
    "train_df['index'] = train_df.index                                          # add new column index\n",
    "columns = ['index', 'Class', 'Sentence']\n",
    "train_df = pre.preprocess_pandas(train_df, columns)                            # pre-process\n",
    "\n",
    "training_data, validation_data, training_labels, validation_labels = pre.train_test_split( # split the data into training, validation, and test splits\n",
    "    train_df['Sentence'].values.astype('U'),\n",
    "    train_df['Class'].values.astype('int32'),\n",
    "    test_size=0.10,\n",
    "    random_state=0,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_df.columns = ['Sentence', 'Class']\n",
    "test_df['index'] = test_df.index                                          # add new column index\n",
    "columns = ['index', 'Class', 'Sentence']\n",
    "test_df = pre.preprocess_pandas(test_df, columns) \n",
    "\n",
    "test_data = test_df['Sentence'].values.astype('U')\n",
    "test_labels = test_df['Class'].values.astype('int32')\n",
    "\n",
    "\n",
    "# vectorize data using TFIDF and transform for PyTorch for scalability\n",
    "word_vectorizer = pre.TfidfVectorizer(analyzer='word', ngram_range=(1,2), max_features=50000, max_df=0.5, use_idf=True, norm='l2')\n",
    "training_data = word_vectorizer.fit_transform(training_data)        # transform texts to sparse matrix\n",
    "training_data = training_data.todense()                             # convert to dense matrix for Pytorch\n",
    "vocab_size = len(word_vectorizer.vocabulary_)\n",
    "\n",
    "validation_data = word_vectorizer.transform(validation_data)\n",
    "validation_data = validation_data.todense()\n",
    "\n",
    "test_data = word_vectorizer.transform(test_data)        # transform texts to sparse matrix\n",
    "test_data = test_data.todense()                             # convert to dense matrix for Pytorch\n",
    "\n",
    "assert training_data.shape[1] == validation_data.shape[1] == test_data.shape[1], \"Feature mismatch in TF-IDF!\"\n",
    "\n",
    "train_x_tensor = torch.from_numpy(np.array(training_data)).type(torch.FloatTensor)\n",
    "train_y_tensor = torch.from_numpy(np.array(training_labels)).long()\n",
    "\n",
    "validation_x_tensor = torch.from_numpy(np.array(validation_data)).type(torch.FloatTensor)\n",
    "validation_y_tensor = torch.from_numpy(np.array(validation_labels)).long()\n",
    "\n",
    "test_x_tensor = torch.from_numpy(np.array(test_data)).type(torch.FloatTensor)\n",
    "test_y_tensor = torch.from_numpy(np.array(test_labels)).long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 5997])\n",
      "torch.Size([720, 5997])\n",
      "torch.Size([80, 5997])\n"
     ]
    }
   ],
   "source": [
    "print(test_x_tensor.shape)\n",
    "print(train_x_tensor.shape)\n",
    "print(validation_x_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "        nn.Linear(vocab_size,1000),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(1000,100),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100,25),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(25,2)\n",
    "        )\n",
    "    \n",
    "    def feedforward(self,input):\n",
    "        return self.network(input)\n",
    "    \n",
    "    \n",
    "def backward(model, epochs, optimizer, loss_function, train_x_tensor, train_y_tensor, validation_x_tensor, validation_y_tensor, device='cuda'):\n",
    "    writer = SummaryWriter(log_dir=\"/tf/logs\")\n",
    "    model.to(device)\n",
    "    train_Acc = 0\n",
    "    train_Acc = 0\n",
    "    validation_AccV = 0\n",
    "    best_loss = 100\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        trainCorr = 0\n",
    "        valCorr = 0\n",
    "        \n",
    "        for (sentences, labels) in zip(train_x_tensor,train_y_tensor):\n",
    "            sentences, labels = sentences.to(device), labels.to(device)\n",
    "            pred = model.feedforward(sentences)\n",
    "            \n",
    "            loss = loss_function(pred, labels) \n",
    "            writer.add_scalar(\"Loss/train\", loss.item(), epoch)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "                \n",
    "            pred = torch.argmax(F.softmax(pred), dim = 0)\n",
    "            trainCorr += torch.sum(pred==labels).item()\n",
    "        train_Acc = trainCorr/len(training_data)\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():   \n",
    "            for (sentencesv, labelsv) in zip(validation_x_tensor,validation_y_tensor):\n",
    "                sentencesv, labelsv = sentencesv.to(device), labelsv.to(device)\n",
    "                predv = model.feedforward(sentencesv)\n",
    "                lossv = loss_function(predv,labelsv)\n",
    "                writer.add_scalar(\"Loss/validation\", loss.item(), epoch)\n",
    "\n",
    "                if best_loss > lossv.item():\n",
    "                    best_loss = lossv.item()\n",
    "                    torch.save(model, \"nlpbest\")\n",
    "                \n",
    "                predv = torch.argmax(F.softmax(predv), dim = 0)\n",
    "                valCorr += torch.sum(predv==labelsv).item()\n",
    "            validation_AccV = valCorr/len(validation_data)\n",
    "        print(\"epoch:\", epoch, \"Loss Training:\", loss.item(), \"Training acc:\", train_Acc)\n",
    "        print(\"epoch:\", epoch, \"Loss Validation:\", lossv.item(),\"Validation acc:\", validation_AccV)\n",
    "        writer.flush()\n",
    "    writer.close()\n",
    "    return model\n",
    "        \n",
    "def test_loop(model, loss_function, test_x_tensor, test_y_tensor, device='cuda'):\n",
    "    model.to(device)\n",
    "    testCorr = 0\n",
    "    model.eval()\n",
    "    best_loss = 100\n",
    "    with torch.no_grad():   \n",
    "        for (sentencesTest, labelsTest) in zip(test_x_tensor,test_y_tensor):\n",
    "            sentencesTest, labelsTest = sentencesTest.to(device), labelsTest.to(device)\n",
    "            predTest = model.feedforward(sentencesTest)\n",
    "            lossTest = loss_function(predTest,labelsTest)\n",
    "\n",
    "            if best_loss > lossTest.item():\n",
    "                best_loss = lossTest.item()\n",
    "            \n",
    "            \n",
    "            predTest = torch.argmax(F.softmax(predTest), dim = 0)\n",
    "            testCorr += torch.sum(predTest==labelsTest).item()\n",
    "        test_Acc = testCorr/len(test_data)\n",
    "    print( \"Loss Test:\", lossTest.item(), \"Test acc:\", test_Acc)\n",
    "\n",
    "    \n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemntation:\n",
    "\n",
    "Small data set Amaxon cell labelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18089/71405532.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = torch.argmax(F.softmax(pred))\n",
      "/tmp/ipykernel_18089/71405532.py:56: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  predv = torch.argmax(F.softmax(predv))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 Loss Training: 0.16985417902469635 Training acc: 0.6805555555555556\n",
      "epoch: 0 Loss Validation: 0.45168471336364746 Validation acc: 0.8\n",
      "epoch: 1 Loss Training: 5.960462772236497e-07 Training acc: 0.9708333333333333\n",
      "epoch: 1 Loss Validation: 0.005050757434219122 Validation acc: 0.7875\n",
      "epoch: 2 Loss Training: -0.0 Training acc: 0.9986111111111111\n",
      "epoch: 2 Loss Validation: 0.014861115254461765 Validation acc: 0.8\n",
      "Loss Test: 3.576272320060525e-06 Test acc: 0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18089/71405532.py:80: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  predTest = torch.argmax(F.softmax(predTest))\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "model = Net()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.001, weight_decay=0.2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "trained_model = backward(model, epochs, optimizer, criterion, train_x_tensor, train_y_tensor, validation_x_tensor, validation_y_tensor)\n",
    "\n",
    "test_loop(trained_model, criterion,test_x_tensor, test_y_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on large data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " # get data, pre-process and split\n",
    "data = pre.pd.read_csv(\"amazon_cells_labelled_LARGE_25K.txt\", delimiter='\\t', header=None)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Split the DataFrame into 80% train and 20% test\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "#training and validation\n",
    "train_df.columns = ['Sentence', 'Class']\n",
    "train_df['index'] = train_df.index                                          # add new column index\n",
    "columns = ['index', 'Class', 'Sentence']\n",
    "train_df = pre.preprocess_pandas(train_df, columns)                            # pre-process\n",
    "\n",
    "training_data, validation_data, training_labels, validation_labels = pre.train_test_split( # split the data into training, validation, and test splits\n",
    "    train_df['Sentence'].values.astype('U'),\n",
    "    train_df['Class'].values.astype('int32'),\n",
    "    test_size=0.10,\n",
    "    random_state=0,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_df.columns = ['Sentence', 'Class']\n",
    "test_df['index'] = test_df.index                                          # add new column index\n",
    "columns = ['index', 'Class', 'Sentence']\n",
    "test_df = pre.preprocess_pandas(test_df, columns) \n",
    "\n",
    "test_data = test_df['Sentence'].values.astype('U')\n",
    "test_labels = test_df['Class'].values.astype('int32')\n",
    "\n",
    "\n",
    "# vectorize data using TFIDF and transform for PyTorch for scalability\n",
    "word_vectorizer = pre.TfidfVectorizer(analyzer='word', ngram_range=(1,2), max_features=50000, max_df=0.5, use_idf=True, norm='l2')\n",
    "training_data = word_vectorizer.fit_transform(training_data)        # transform texts to sparse matrix\n",
    "training_data = training_data.todense()                             # convert to dense matrix for Pytorch\n",
    "vocab_size = len(word_vectorizer.vocabulary_)\n",
    "\n",
    "validation_data = word_vectorizer.transform(validation_data)\n",
    "validation_data = validation_data.todense()\n",
    "\n",
    "test_data = word_vectorizer.transform(test_data)        # transform texts to sparse matrix\n",
    "test_data = test_data.todense()                             # convert to dense matrix for Pytorch\n",
    "\n",
    "assert training_data.shape[1] == validation_data.shape[1] == test_data.shape[1], \"Feature mismatch in TF-IDF!\"\n",
    "\n",
    "train_x_tensor = torch.from_numpy(np.array(training_data)).type(torch.FloatTensor)\n",
    "train_y_tensor = torch.from_numpy(np.array(training_labels)).long()\n",
    "\n",
    "validation_x_tensor = torch.from_numpy(np.array(validation_data)).type(torch.FloatTensor)\n",
    "validation_y_tensor = torch.from_numpy(np.array(validation_labels)).long()\n",
    "\n",
    "test_x_tensor = torch.from_numpy(np.array(test_data)).type(torch.FloatTensor)\n",
    "test_y_tensor = torch.from_numpy(np.array(test_labels)).long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18000])\n"
     ]
    }
   ],
   "source": [
    "print(train_y_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18089/71405532.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = torch.argmax(F.softmax(pred))\n",
      "/tmp/ipykernel_18089/71405532.py:56: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  predv = torch.argmax(F.softmax(predv))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 Loss Training: 1.1358058452606201 Training acc: 0.8613888888888889\n",
      "epoch: 0 Loss Validation: 1.326241374015808 Validation acc: 0.8925\n",
      "epoch: 1 Loss Training: 0.41631880402565 Training acc: 0.9622777777777778\n",
      "epoch: 1 Loss Validation: 0.033916376531124115 Validation acc: 0.8895\n",
      "epoch: 2 Loss Training: 0.024427037686109543 Training acc: 0.9917777777777778\n",
      "epoch: 2 Loss Validation: 3.765075206756592 Validation acc: 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18089/71405532.py:80: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  predTest = torch.argmax(F.softmax(predTest))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Test: 0.00029797881143167615 Test acc: 0.8796\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "model = Net()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "trained_model = backward(model, epochs, optimizer, criterion, train_x_tensor, train_y_tensor, validation_x_tensor, validation_y_tensor)\n",
    "test_loop(trained_model, criterion,test_x_tensor, test_y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        2\n",
      "1        1\n",
      "2        2\n",
      "3        1\n",
      "4        0\n",
      "        ..\n",
      "31227    0\n",
      "31228    1\n",
      "31229    2\n",
      "31230    1\n",
      "31231    0\n",
      "Name: Class, Length: 31232, dtype: int64\n",
      "0       0\n",
      "1       2\n",
      "2       2\n",
      "3       2\n",
      "4       2\n",
      "       ..\n",
      "5200    1\n",
      "5201    2\n",
      "5202    0\n",
      "5203    2\n",
      "5204    2\n",
      "Name: Class, Length: 5205, dtype: int64\n",
      "0       1\n",
      "1       2\n",
      "2       0\n",
      "3       1\n",
      "4       2\n",
      "       ..\n",
      "5201    0\n",
      "5202    1\n",
      "5203    0\n",
      "5204    2\n",
      "5205    2\n",
      "Name: Class, Length: 5206, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def preprocess_pandas(data, columns):\n",
    "    df_ = pd.DataFrame(columns=columns)\n",
    "    data['Sentence'] = data['Sentence'].str.lower()\n",
    "    data['Sentence'] = data['Sentence'].replace('[a-zA-Z0-9-_.]+@[a-zA-Z0-9-_.]+', '', regex=True)                      # remove emails\n",
    "    data['Sentence'] = data['Sentence'].replace('((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.|$)){4}', '', regex=True)    # remove IP address\n",
    "    data['Sentence'] = data['Sentence'].str.replace('[^\\w\\s]','')                                                       # remove special characters\n",
    "    data['Sentence'] = data['Sentence'].replace('\\d', '', regex=True)                                                   # remove numbers\n",
    "    for index, row in data.iterrows():\n",
    "        word_tokens = word_tokenize(str(row['Sentence']))\n",
    "        filtered_sent = [w for w in word_tokens if not w in stopwords.words('english')]\n",
    "        df_.loc[len(df_)] = {\n",
    "            \"index\": row['index'],\n",
    "            \"Class\": row['Class'],\n",
    "            \"Sentence\": \" \".join(filtered_sent)\n",
    "        }\n",
    "  \n",
    "    return df_\n",
    "\n",
    "# get data, pre-process and split\n",
    "train_df = pre.pd.read_csv(\"train_df.csv\", delimiter=',')\n",
    "val_df = pre.pd.read_csv(\"val_df.csv\", delimiter=',')\n",
    "test_df = pre.pd.read_csv(\"test_df.csv\", delimiter=',')\n",
    "\n",
    "#training and validation\n",
    "train_df.columns = ['index', 'Sentence', 'Class', 'Label']\n",
    "train_df['index'] = train_df.index    # add new column index\n",
    "columns = ['index', 'Sentence','Class', 'Label']\n",
    "train_df = preprocess_pandas(train_df, columns)                            # pre-process\n",
    "print(train_df['Class'])\n",
    "\n",
    "val_df.columns = ['index','Sentence','Class','Label']\n",
    "val_df['index'] = val_df.index    # add new column index\n",
    "columns = ['index', 'Sentence','Class', 'Label']\n",
    "val_df = preprocess_pandas(val_df, columns)                                 # pre-process\n",
    "print(val_df['Class'])\n",
    "\n",
    "test_df.columns = ['index','Sentence','Class', 'Label']\n",
    "test_df['index'] = test_df.index    # add new column index\n",
    "columns = ['index', 'Sentence','Class', 'Label']                            # pre-process\n",
    "test_df = preprocess_pandas(test_df, columns) \n",
    "print(test_df['Class'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "training_data = train_df['Sentence'].values.astype('U')\n",
    "training_labels = train_df['Class'].values.astype('int32')\n",
    "\n",
    "validation_data = val_df['Sentence'].values.astype('U')\n",
    "validation_labels = val_df['Class'].values.astype('int32')\n",
    "\n",
    "test_data = test_df['Sentence'].values.astype('U')\n",
    "test_labels = test_df['Class'].values.astype('int32')\n",
    "\n",
    "# vectorize data using TFIDF and transform for PyTorch for scalability\n",
    "word_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,2), max_features=50000, max_df=0.5, use_idf=True, norm='l2')\n",
    "training_data = word_vectorizer.fit_transform(training_data)        # transform texts to sparse matrix\n",
    "training_data = training_data.todense()                       # convert to dense matrix for Pytorch\n",
    "vocab_size = len(word_vectorizer.vocabulary_)\n",
    "\n",
    "validation_data = word_vectorizer.transform(validation_data)\n",
    "validation_data = validation_data.todense()   \n",
    " \n",
    "test_data = word_vectorizer.transform(test_data)        # transform texts to sparse matrix\n",
    "test_data = test_data.todense()                              # convert to dense matrix for Pytorch\n",
    "\n",
    "assert training_data.shape[1] == validation_data.shape[1] == test_data.shape[1], \"Feature mismatch in TF-IDF!\"\n",
    "print(training_data)\n",
    "\n",
    "train_x_tensor = torch.from_numpy(training_data).type(torch.FloatTensor)\n",
    "train_y_tensor = torch.from_numpy(np.array(training_labels)).long()\n",
    "\n",
    "validation_x_tensor = torch.from_numpy(validation_data).type(torch.FloatTensor)\n",
    "validation_y_tensor = torch.from_numpy(np.array(validation_labels)).long()\n",
    "\n",
    "test_x_tensor = torch.from_numpy(test_data).type(torch.FloatTensor)\n",
    "test_y_tensor = torch.from_numpy(np.array(test_labels)).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "        nn.Linear(vocab_size,1000),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(1000,100),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100,25),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(25,3)\n",
    "        )\n",
    "    \n",
    "    def feedforward(self,input):\n",
    "        return self.network(input)\n",
    "    \n",
    "    \n",
    "def backward(model, epochs, optimizer, loss_function, train_x_tensor, train_y_tensor, validation_x_tensor, validation_y_tensor, device='cuda'):\n",
    "    writer = SummaryWriter(log_dir=\"/tf/logs\")\n",
    "    model.to(device)\n",
    "    train_Acc = 0\n",
    "    train_Acc = 0\n",
    "    validation_AccV = 0\n",
    "    best_loss = 100\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        trainCorr = 0\n",
    "        valCorr = 0\n",
    "        \n",
    "        for (sentences, labels) in zip(train_x_tensor,train_y_tensor):\n",
    "            sentences, labels = sentences.to(device), labels.to(device)\n",
    "            pred = model.feedforward(sentences)\n",
    "            \n",
    "            loss = loss_function(pred, labels)\n",
    "            writer.add_scalar(\"Loss/train\", loss.item(), epoch)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "                \n",
    "            pred = torch.argmax(F.softmax(pred), dim = 0)\n",
    "            trainCorr += torch.sum(pred==labels).item()\n",
    "        train_Acc = trainCorr/len(training_data)\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():   \n",
    "            for (sentencesv, labelsv) in zip(validation_x_tensor,validation_y_tensor):\n",
    "                sentencesv, labelsv = sentencesv.to(device), labelsv.to(device)\n",
    "                predv = model.feedforward(sentencesv)\n",
    "                lossv = loss_function(predv,labelsv)\n",
    "                writer.add_scalar(\"Loss/validation\", loss.item(), epoch)\n",
    "\n",
    "                if best_loss > lossv.item():\n",
    "                    best_loss = lossv.item()\n",
    "                    torch.save(model, \"nlpbest\")\n",
    "                \n",
    "                predv = torch.argmax(F.softmax(predv), dim = 0)\n",
    "                valCorr += torch.sum(predv==labelsv).item()\n",
    "            validation_AccV = valCorr/len(validation_data)\n",
    "        print(\"epoch:\", epoch, \"Loss Training:\", loss.item(), \"Training acc:\", train_Acc)\n",
    "        print(\"epoch:\", epoch, \"Loss Validation:\", lossv.item(),\"Validation acc:\", validation_AccV)\n",
    "        writer.flush()\n",
    "    writer.close()\n",
    "    return model\n",
    "        \n",
    "def test_loop(model, loss_function, test_x_tensor, test_y_tensor, device='cuda'):\n",
    "    model.to(device)\n",
    "    testCorr = 0\n",
    "    model.eval()\n",
    "    best_loss = 100\n",
    "    with torch.no_grad():   \n",
    "        for (sentencesTest, labelsTest) in zip(test_x_tensor,test_y_tensor):\n",
    "            sentencesTest, labelsTest = sentencesTest.to(device), labelsTest.to(device)\n",
    "            predTest = model.feedforward(sentencesTest)\n",
    "            lossTest = loss_function(predTest,labelsTest)\n",
    "\n",
    "            if best_loss > lossTest.item():\n",
    "                best_loss = lossTest.item()\n",
    "            \n",
    "            \n",
    "            predTest = torch.argmax(F.softmax(predTest), dim = 0)\n",
    "            testCorr += torch.sum(predTest==labelsTest).item()\n",
    "        test_Acc = testCorr/len(test_data)\n",
    "    print( \"Loss Test:\", lossTest.item(), \"Test acc:\", test_Acc)\n",
    "\n",
    "    \n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18089/3548125506.py:81: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  predTest = torch.argmax(F.softmax(predTest), dim = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Test: 2.95444655418396 Test acc: 0.629850172877449\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "model = Net()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#trained_model = backward(model, epochs, optimizer, criterion, train_x_tensor, train_y_tensor, validation_x_tensor, validation_y_tensor)\n",
    "trained_model = torch.load(\"nlpbest\", weights_only = False)\n",
    "test_loop(trained_model, criterion,test_x_tensor, test_y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
