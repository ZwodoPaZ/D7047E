{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84459d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 17:03:18.887336: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744729398.906238   60406 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744729398.911970   60406 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744729398.927121   60406 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744729398.927139   60406 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744729398.927141   60406 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744729398.927143   60406 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-15 17:03:18.932448: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BertConfig, BertModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import copy\n",
    "import time\n",
    "\n",
    "class TokenData(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        # data = (train_x, train_tokens, train_y)#\n",
    "        self.text_data = data[0]\n",
    "        self.labels = data[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        text = str(self.text_data[idx])\n",
    "        label = 0 if self.labels[idx] == 1 else 1\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Return input_ids, attention_mask, and label\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item['labels'] = label\n",
    "        return item\n",
    "        \n",
    "def regexPreProcessing(data):\n",
    "    data.text = data.text.astype('str')\n",
    "    data.text = data.text.str.lower()\n",
    "    data.text = data.text.replace('[a-zA-Z0-9-_.]+@[a-zA-Z0-9-_.]+', '', regex=True)                      # remove emails\n",
    "    data.text = data.text.replace('((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.|$)){4}', '', regex=True)    # remove IP address\n",
    "    data.text = data.text.str.replace('[^\\w\\s]','')                                                       # remove special characters\n",
    "    data.text = data.text.replace('\\d', '', regex=True)\n",
    "    return list(data.text), list(data.label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc6b8afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels']) 3\n",
      "['[CLS]', 'avoid', 'like', 'the', 'plague', '!', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "tensor([  101, 12476,   999,   102,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n",
      "training_loader_size: 19200 validation_loader_size: 4800 test_loader_size: 2667\n"
     ]
    }
   ],
   "source": [
    "# For bert transformer #\n",
    "# Code for provided datasets using amazon reviews #\n",
    "\n",
    "\"\"\"data = pd.read_csv(\"amazon_cells_labelled.txt\", delimiter='\\t', header=None)\n",
    "data = pd.read_csv(\"amazon_cells_labelled_LARGE_25K.txt\", delimiter='\\t', header=None)#\n",
    "data.columns = ['Sentence', 'Class']\n",
    "data['index'] = data.index                                          # add new column index\n",
    "columns = ['index', 'Class', 'Sentence']\n",
    "data = preprocess_pandas(data, columns)                             # pre-process\n",
    "training_data, validation_data, training_labels, validation_labels = train_test_split( # split the data into training, validation, and test splits\n",
    "    data['Sentence'].values.astype('U'),\n",
    "    data['Class'].values.astype('int32'),\n",
    "    test_size=0.10,\n",
    "    random_state=0,\n",
    "    shuffle=True\n",
    ")\n",
    "training_data = list(training_data)\n",
    "testing_data = list(validation_data)\n",
    "training_labels = list(training_labels)\n",
    "testing_labels = list(validation_labels)\"\"\"\n",
    "\n",
    "# Code for twitter dataset #\n",
    "\n",
    "\"\"\"data = pd.read_csv(\"twitter_training.csv\", delimiter=',', header=None)#\n",
    "data.columns = ['id', 'context', 'Sentence', 'Class']\n",
    "data['index'] = data.index                                          # add new column index\n",
    "columns = ['id', 'context' ,'Class', 'Sentence', 'index']                      # pre-process\n",
    "training_data, validation_data, training_labels, validation_labels = train_test_split( # split the data into training, validation, and test splits\n",
    "    data['Sentence'].values.astype('U'),\n",
    "    data['Class'].values.astype('U'),\n",
    "    test_size=0.10,\n",
    "    random_state=0,\n",
    "    shuffle=True\n",
    ")\n",
    "training_data = list(training_data)\n",
    "validation_data = list(validation_data)\n",
    "training_labels = list(training_labels)\n",
    "training_labels = [0 if v == 'Positive' else 1 if v == 'Negative' else 2 if v == 'Neutral' else 3 for v in training_labels]\n",
    "validation_labels = list(validation_labels)\n",
    "validation_labels = [0 if v == 'Positive' else 1 if v == 'Negative' else 2 if v == 'Neutral' else 3 for v in validation_labels]\n",
    "\n",
    "data = pd.read_csv(\"twitter_test.csv\", delimiter=',', header=None)#\n",
    "data.columns = ['id', 'context', 'Class', 'Sentence']\n",
    "data['index'] = data.index                                          # add new column index\n",
    "columns = ['id', 'context' ,'Class', 'Sentence', 'index']     \n",
    "\n",
    "testing_data = list(data['Sentence'])\n",
    "testing_labels = list(data['Class'])\n",
    "testing_labels = [0 if v == 'Positive' else 1 if v == 'Negative' else 2 if v == 'Neutral' else 3 for v in testing_labels]\"\"\"\n",
    "\n",
    "\"\"\"# Code for hugging face dataset - multi category sentiment analysis\n",
    "\n",
    "data = pd.read_csv(\"train_df.csv\", delimiter=',')\n",
    "training_data, training_labels = regexPreProcessing(data) \n",
    "\n",
    "\n",
    "data = pd.read_csv(\"val_df.csv\", delimiter=',')\n",
    "validation_data, validation_labels = regexPreProcessing(data)\n",
    "\n",
    "data = pd.read_csv(\"test_df.csv\", delimiter=',')\n",
    "testing_data, testing_labels = regexPreProcessing(data)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "train_data = (training_data, tokenizer(training_data, padding = True, truncation=True, return_tensors=\"pt\"), training_labels)\n",
    "val_data = (validation_data, tokenizer(validation_data, padding = True,  truncation=True, return_tensors=\"pt\"), validation_labels)\n",
    "test_data = (testing_data, tokenizer(testing_data, padding = True, truncation=True, return_tensors=\"pt\"), testing_labels)\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "train_dataset = TokenData(train_data, train = True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "val_dataset = TokenData(val_data, train = True)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "test_dataset = TokenData(test_data ,train = False)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "print(\"training_loader_size:\", len(train_loader), \"validation_loader_size:\", len(val_loader), \"test_loader_size:\", len(test_loader))\"\"\"\n",
    "\n",
    "# Code for massive dataset #\n",
    "\n",
    "batch_size = 150\n",
    "data = pd.read_csv(\"train_massive.csv\", delimiter=',', header=None)\n",
    "data.columns = ['Class', 'Sentence', 'Body']\n",
    "train_data = list(data['Sentence'])\n",
    "train_labels = list(data['Class'])\n",
    "train_data = (train_data, train_labels)\n",
    "# Regular BERT uncased\n",
    "#train_val_dataset = TokenData(train_data, tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\"))\n",
    "\n",
    "# DistilBERT uncased\n",
    "train_val_dataset = TokenData(train_data, tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\"))\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_val_dataset, [0.8, 0.2])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "data = pd.read_csv(\"test_massive.csv\", delimiter=',', header=None)\n",
    "data.columns = ['Class', 'Sentence', 'Body']\n",
    "testing_data = list(data['Sentence'])\n",
    "testing_labels = list(data['Class'])\n",
    "test_data = (testing_data, testing_labels)\n",
    "\n",
    "# Regular BERT uncased\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "#test_dataset = TokenData(test_data, tokenizer = tokenizer)\n",
    "\n",
    "# DistilBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "test_dataset = TokenData(test_data, tokenizer = tokenizer)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, shuffle=True, batch_size=batch_size)\n",
    "print(next(enumerate(test_loader))[1].keys(), len(next(enumerate(test_loader))[1]))\n",
    "print(tokenizer.convert_ids_to_tokens(next(enumerate(test_loader))[1]['input_ids'][0], skip_special_tokens=False))\n",
    "print(next(enumerate(test_loader))[1]['input_ids'][0])\n",
    "print(\"training_loader_size:\", len(train_loader), \"validation_loader_size:\", len(val_loader), \"test_loader_size:\", len(test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "516adeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular BERT training loop #\n",
    "\n",
    "def train_model_bert(model, criterion, optimizer, train_loader, val_loader, num_epochs, batch_size, device):\n",
    "    # For each epoch\n",
    "    tensorboard_logging_path = \"/tf/logs/BertTransformer_Massive2\" + str(time.time())\n",
    "    writer = SummaryWriter(log_dir=tensorboard_logging_path)\n",
    "    best_validation_loss = float(\"inf\")\n",
    "    best_training_loss = float(\"inf\")\n",
    "    best_training_accuracy = 0\n",
    "    best_validation_accuracy = 0\n",
    "    for i in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        total_validation_loss = 0\n",
    "        total_training_loss = 0 \n",
    "        correct_train = 0\n",
    "        correct_validation = 0\n",
    "        for batch_nr, batch in enumerate(train_loader):\n",
    "            \n",
    "            input_ids, token_type_ids, attention_mask, labels = batch.values()\n",
    "            \n",
    "            input_ids = input_ids.to(device)\n",
    "            token_type_ids = token_type_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.long().to(device)\n",
    "            output = model(input_ids = input_ids, attention_mask = attention_mask) \n",
    "            prediction = output.logits\n",
    "            training_loss = criterion(prediction, labels)\n",
    "            writer.add_scalar('Loss Batch / Train', training_loss, i*batch_size + batch_nr)\n",
    "            writer.flush()\n",
    "            total_training_loss += training_loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            training_loss.backward()\n",
    "           \n",
    "            optimizer.step() \n",
    "                \n",
    "            #Print the epoch, batch, and loss\n",
    "            print(\n",
    "                '\\rEpoch {} [{}/{}] - Loss: {}'.format(\n",
    "                    i+1, batch_nr+1, len(train_loader), training_loss\n",
    "                ),\n",
    "                end=''\n",
    "            )\n",
    "        print()\n",
    "            \n",
    "        writer.add_scalar('Loss/Train', total_training_loss, i)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for validation_nr, batch in enumerate(val_loader):\n",
    "                \n",
    "                input_ids, token_type_ids, attention_mask, labels = batch.values()\n",
    "            \n",
    "                input_ids = input_ids.to(device)\n",
    "                token_type_ids = token_type_ids.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                labels = labels.long().to(device)\n",
    "                \n",
    "                output = model(input_ids = input_ids, attention_mask = attention_mask) \n",
    "                prediction = output.logits\n",
    "                \n",
    "                validation_loss = criterion(prediction, labels)\n",
    "                writer.add_scalar('Loss Batch / Validation', validation_loss, i*batch_size + validation_nr)\n",
    "                writer.flush()\n",
    "                total_validation_loss += validation_loss.item()\n",
    "                \n",
    "            if total_validation_loss < best_validation_loss:\n",
    "                torch.save(model, \"./models/BertTransformer_Massive2\")\n",
    "                best_validation_loss = total_validation_loss\n",
    "        del input_ids, token_type_ids, attention_mask, labels, prediction, output, validation_loss, training_loss\n",
    "        writer.add_scalar(\"Loss/Validation\", total_validation_loss, i)\n",
    "        writer.flush()\n",
    "    \n",
    "    writer.close()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a09ab91d-b4fe-4caf-a165-ebc6caf72a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distil BERT training loop #\n",
    "\n",
    "def train_model_destil(model, criterion, optimizer, train_loader, val_loader, num_epochs, batch_size, device):\n",
    "    # For each epoch\n",
    "    tensorboard_logging_path = \"/tf/logs/BertTransformer_MassiveDistil1\" + str(time.time())\n",
    "    writer = SummaryWriter(log_dir=tensorboard_logging_path)\n",
    "    best_validation_loss = float(\"inf\")\n",
    "    best_training_loss = float(\"inf\")\n",
    "    best_training_accuracy = 0\n",
    "    best_validation_accuracy = 0\n",
    "    for i in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        total_validation_loss = 0\n",
    "        total_training_loss = 0 \n",
    "        correct_train = 0\n",
    "        correct_validation = 0\n",
    "        for batch_nr, batch in enumerate(train_loader):\n",
    "            \n",
    "            input_ids, attention_mask, labels = batch.values()\n",
    "            \n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.long().to(device)\n",
    "            output = model(input_ids = input_ids, attention_mask = attention_mask) \n",
    "            prediction = output.logits\n",
    "            training_loss = criterion(prediction, labels)\n",
    "            writer.add_scalar('Loss Batch / Train', training_loss, i*batch_size + batch_nr)\n",
    "            writer.flush()\n",
    "            total_training_loss += training_loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            training_loss.backward()\n",
    "           \n",
    "            optimizer.step() \n",
    "                \n",
    "            #Print the epoch, batch, and loss\n",
    "            print(\n",
    "                '\\rEpoch {} [{}/{}] - Loss: {}'.format(\n",
    "                    i+1, batch_nr+1, len(train_loader), training_loss\n",
    "                ),\n",
    "                end=''\n",
    "            )\n",
    "        print()\n",
    "            \n",
    "        writer.add_scalar('Loss/Train', total_training_loss, i)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for validation_nr, batch in enumerate(val_loader):\n",
    "                \n",
    "                input_ids, attention_mask, labels = batch.values()\n",
    "            \n",
    "                input_ids = input_ids.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                labels = labels.long().to(device)\n",
    "                \n",
    "                output = model(input_ids = input_ids, attention_mask = attention_mask) \n",
    "                prediction = output.logits\n",
    "                \n",
    "                validation_loss = criterion(prediction, labels)\n",
    "                writer.add_scalar('Loss Batch / Validation', validation_loss, i*batch_size + validation_nr)\n",
    "                writer.flush()\n",
    "                total_validation_loss += validation_loss.item()\n",
    "                \n",
    "            if total_validation_loss < best_validation_loss:\n",
    "                torch.save(model, \"./models/BertTransformer_MassiveDistil1\")\n",
    "                best_validation_loss = total_validation_loss\n",
    "        del input_ids, attention_mask, labels, prediction, output, validation_loss, training_loss\n",
    "        writer.add_scalar(\"Loss/Validation\", total_validation_loss, i)\n",
    "        writer.flush()\n",
    "    \n",
    "    writer.close()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4e4a894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regular BERT Base Uncased #\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 2) # Pre-trained model\n",
    "model.dropout.p = 0.3\n",
    "optimizer = AdamW(model.parameters(), lr=5e-6, weight_decay = 0.01) # Optimization function\n",
    "criterion = torch.nn.CrossEntropyLoss() # Loss function\n",
    "epochs = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device) # Transfer model to GPU if available\n",
    "#trained_model = train_model_bert(model, criterion, optimizer, train_loader, val_loader, epochs, batch_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5f6f485-c3e7-4bfb-af81-cb58fe64fd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [19200/19200] - Loss: 0.23904785513877873\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'token_type_ids' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m model.to(device) \u001b[38;5;66;03m# Transfer model to GPU if available\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m trained_model = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 69\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, criterion, optimizer, train_loader, val_loader, num_epochs, batch_size, device)\u001b[39m\n\u001b[32m     67\u001b[39m         torch.save(model, \u001b[33m\"\u001b[39m\u001b[33m./models/BertTransformer_MassiveDistil1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     68\u001b[39m         best_validation_loss = total_validation_loss\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m input_ids, \u001b[43mtoken_type_ids\u001b[49m, attention_mask, labels, prediction, output, validation_loss, training_loss\n\u001b[32m     70\u001b[39m writer.add_scalar(\u001b[33m\"\u001b[39m\u001b[33mLoss/Validation\u001b[39m\u001b[33m\"\u001b[39m, total_validation_loss, i)\n\u001b[32m     71\u001b[39m writer.flush()\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'token_type_ids' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "# DistilBERT Uncased #\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels = 2) # Pre-trained model\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay = 0.01) # Optimization function\n",
    "criterion = torch.nn.CrossEntropyLoss() # Loss function\n",
    "epochs = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device) # Transfer model to GPU if available\n",
    "trained_model = train_model_destil(model, criterion, optimizer, train_loader, val_loader, epochs, batch_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9cac271a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress 2666Testing accuracy:  0.8964425\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"./models/BertTransformer_MassiveDistil1\",weights_only = False)\n",
    "#model = torch.load(\"./models/BertTransformerThingV1NoPreTraining\",weights_only = False)\n",
    "model.to(device)\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for test_nr, batch in enumerate(test_loader):\n",
    "                \n",
    "        input_ids, attention_mask, labels = batch.values()\n",
    "    \n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        output = model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "        \n",
    "        logits = output.logits\n",
    "    \n",
    "        correct += (logits.argmax(1) == labels).sum().item())\n",
    "        total += labels.size(0)\n",
    "        print('\\rProgress {}'.format(test_nr), end = '')\n",
    "print(\"Testing accuracy: \",correct/(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578bf6ff-eb38-45d5-afef-fe6248962ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
